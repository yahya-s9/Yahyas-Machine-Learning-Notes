https://arxiv.org/abs/2510.17269

Speaker: Nick

- hugging face put together a new vision dataset
	- 24 million samples

- paper talks ab which data sets htey use, how they build pipeline that takes in 200 different datasets with different formats and still standardize them using claude

- cleaning and dup using embedding, cosine similarity, throw away ones below a treshold

- evaluating different datasets

- questions ur asking are close to what is out there on the internet, so llm not that smart, its an incredible data sponge, model will be triggered by the internal represenation of the question